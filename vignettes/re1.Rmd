---
title: "Report 1: Digital Soil Mapping"
author: "ETadiri"
date: "2025-12-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
library(dplyr); library(readr); library(purrr); library(ggplot2); library(terra);
library(tidyterra); library(leaflet); library(sf); library(here); library(knitr); 
library(tidyr); library(ranger); library(parallel); library(renv); library(purrr); 
library(stringr); library(pROC)
```

# 1. DATA PREPARATION
## 1.1 Load data
### 1.1.1 Soil samples

```{r}
# Load soil data from sampling locations
df_obs <- readr::read_csv(
  here::here("data-raw/soildata/berne_soil_sampling_locations.csv")
  )

# Display data
head(df_obs) |> 
  knitr::kable()
```

### 1.1.2 Environmental covariates
```{r}
# Get a list with the path to all raster files
list_raster <- list.files(
  here::here("data-raw/geodata/covariates/"),
  full.names = TRUE
  )

# Display file names
map(
  list_raster,
  ~ basename(.)
) |>
  head()
```

```{r}
# Load a raster file as example: Picking the slope profile at 2 m resolution
raster_example <- rast(
  here("data-raw/geodata/covariates/Se_slope2m.tif")
)
raster_example
```

## 1.2 Visualize data
```{r}
# Plot raster example
terra::plot(raster_example)
# To have some more flexibility, we can plot this in the ggplot-style as such:
ggplot() +
  tidyterra::geom_spatraster(data = raster_example) +
  scale_fill_viridis_c(
    na.value = NA,
    option = "magma",
    name = "Slope (%) \n"
  ) +
  theme_bw() +
  scale_x_continuous(expand = c(0, 0)) + # avoid gap between plotting area and axis
  scale_y_continuous(expand = c(0, 0)) +
  labs(title = "Slope of the Study Area")
```
```{r}
# To get our map working correctly, we have to ensure that all the input data
# is in the same coordinate system. Since our Bern data is in the Swiss
# coordinate system, we have to transform the sampling locations to the
# World Geodetic System first.
# To look up EPSG Codes: https://epsg.io/
# World Geodetic System 1984:  4326
# Swiss CH1903+ / LV95: 2056

# For the raster:
rasta <- terra::project(raster_example, "+init=EPSG:4326")

# Let's make a function for transforming the sampling locations:
change_coords <- function(data, from_CRS, to_CRS) {
  # Check if data input is correct
  if (!all(names(data) %in% c("id", "lat", "lon"))) {
    stop("Input data needs variables: id, lat, lon")
  }

  # Create simple feature for old CRS
  sf_old_crs <- st_as_sf(data, coords = c("lon", "lat"), crs = from_CRS)

  # Transform to new CRS
  sf_new_crs <- st_transform(sf_old_crs, crs = to_CRS)
  sf_new_crs$lat <- st_coordinates(sf_new_crs)[, "Y"]
  sf_new_crs$lon <- st_coordinates(sf_new_crs)[, "X"]

  sf_new_crs <- sf_new_crs |>
    as_tibble() |>
    dplyr::select(id, lat, lon)

  # Return new CRS
  return(sf_new_crs)
}

# Transform dataframes
coord_train <- df_obs |>
  filter(dataset == "calibration") |>
  dplyr::select(site_id_unique, x, y) |>
  rename(id = site_id_unique, lon = x, lat = y) |>
  change_coords(
    from_CRS = 2056,
    to_CRS = 4326
    )

coord_test <- df_obs |> 
  dplyr::filter(dataset == "validation") |> 
  dplyr::select(site_id_unique, x, y) |> 
  dplyr::rename(id = site_id_unique, lon = x, lat = y) |> 
  change_coords(
    from_CRS = 2056, 
    to_CRS = 4326
    )
```

```{r}
# Notes: 
# - This code may only work when installing the development branch of {leaflet}:
# remotes::install_github('rstudio/leaflet')
# - You might have to do library(terra) for R to find functions needed in the backend


# Let's get a nice color palette now for easy reference
pal <- leaflet::colorNumeric(
  "magma",
  terra::values(rasta),
  na.color = "transparent"
  )

coord_test <- df_obs |>
  filter(dataset == "validation") |>
  dplyr::select(site_id_unique, x, y) |>
  rename(id = site_id_unique, lon = x, lat = y) |>
  change_coords(
    from_CRS = 2056,
    to_CRS = 4326
  )
```

```{r}
# Notes:
# - This code may only work when installing the development branch of {leaflet}:
# remotes::install_github('rstudio/leaflet')

# Let's get a nice color palette now for easy reference
pal <- colorNumeric(
  "magma",
  values(rasta),
  na.color = "transparent"
)

# Next, we build a leaflet map
leaflet() |>
  # As base maps, use two provided by ESRI
  addProviderTiles(providers$Esri.WorldImagery, group = "World Imagery") |>
  addProviderTiles(providers$Esri.WorldTopoMap, group = "World Topo") |>
  # Add our raster file
  addRasterImage(
    rasta,
    colors = pal,
    opacity = 0.6,
    group = "raster"
  ) |>
  # Add markers for sampling locations
  addCircleMarkers(
    data = coord_train,
    lng = ~lon, # Column name for x coordinates
    lat = ~lat, # Column name for y coordinates
    group = "training",
    color = "black"
  ) |>
  addCircleMarkers(
    data = coord_test,
    lng = ~lon, # Column name for x coordinates
    lat = ~lat, # Column name for y coordinates
    group = "validation",
    color = "red"
  ) |>
  # Add some layout and legend
  addLayersControl(
    baseGroups = c("World Imagery", "World Topo"),
    position = "topleft",
    options = layersControlOptions(collapsed = FALSE),
    overlayGroups = c("raster", "training", "validation")
  ) |>
  addLegend(
    pal = pal,
    values = terra::values(rasta),
    title = "Slope (%)")

```

## 1.3 Combine data
```{r}
# Load all files as one batch
all_rasters <- rast(list_raster)
all_rasters
```

```{r}
# Extract coordinates from sampling locations
sampling_xy <- df_obs |> 
  dplyr::select(x, y)

# From all rasters, extract values for sampling coordinates
df_covars <- terra::extract(
  all_rasters,  # The raster we want to extract from
  sampling_xy,  # A matrix of x and y values to extract for
  ID = FALSE    # To not add a default ID column to the output
  )

df_full <- cbind(df_obs, df_covars)

head(df_full) |>
  kable()
```

## 1.4 Data wrangling
```{r}
vars_categorical <- df_covars |>
  # Get number of distinct values per variable
  summarise(across(everything(), ~ n_distinct(.))) |>
  # Turn df into long format for easy filtering
  tidyr::pivot_longer(
    everything(),
    names_to = "variable",
    values_to = "n"
  ) |>
  # Filter out variables with 10 or less distinct values
  filter(n <= 10) |>
  # Extract the names of these variables
  pull("variable")

cat(
  "Variables with less than 10 distinct values:",
  ifelse(length(vars_categorical) == 0, "none", vars_categorical)
)

df_full <- df_full |>
  mutate(
    across(all_of(vars_categorical), ~ as.factor(.)),
    across(contains("waterlog"), ~as.factor(.))
    )
```

## 1.5 Check missing data
```{r}
# Get number of rows to calculate percentages
n_rows <- nrow(df_full)

# Get number of distinct values per variable
df_full |>
  summarise(across(
    everything(),
    ~ length(.) - sum(is.na(.))
  )) |>
  tidyr::pivot_longer(everything(),
    names_to = "variable",
    values_to = "n"
  ) |>
  mutate(perc_available = round(n / n_rows * 100)) |>
  arrange(perc_available) |>
  head(10) |>
  knitr::kable()

df_full |>
  dplyr::select(1:20) |> # reduce data for readability of the plot
  visdat::vis_miss()
```

## 1.6 Save data
``` {r}
if (!dir.exists(here("data/"))) system(paste0("mkdir ", here("data/")))
saveRDS(
  df_full,
  here("data/df_full.rds")
)
```



# 2. TRAIN A RANDOM FOREST
## 2.1 Simple model
### 2.1.1 Load data
```{r}
df_full <- readRDS(here("data/df_full.rds"))

head(df_full) |>
  knitr::kable()
```

### 2.1.2 Preparation
```{r}
# Specify target: waterlog
target <- "waterlog.100"

# Specify predictors_all: Remove soil sampling and observational data
predictors_all <- names(df_full)[14:ncol(df_full)]

cat(
  "The target is:", target,
  "\nThe predictors_all are:", paste0(predictors_all[1:8], sep = ", "), "..."
)
```

```{r}
# Split dataset into training and testing sets
df_train <- df_full |> 
  filter(dataset == "calibration")

df_test <- df_full |> 
  filter(dataset == "validation")

# Filter out any NA to avoid error when running a Random Forest
df_train <- df_train |> 
  drop_na()

df_test <- df_test |> 
  drop_na()

# A little bit of verbose output:
n_tot <- nrow(df_train) + nrow(df_test)

perc_cal <- (nrow(df_train) / n_tot) |> round(2) * 100
perc_val <- (nrow(df_test) / n_tot) |> round(2) * 100

cat(
  "For model training, we have a calibration / validation split of: ",
  perc_cal, "/", perc_val, "%"
)
```

### 2.1.3 Model training
``` {r}
# ranger() crashes when using tibbles, so we are using the
# base R notation to enter the data
rf_basic <- ranger(
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all], # Predictor variables
  seed = 42, # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1,
  probability = T
  ) # Use all but one CPU core for quick model training

# Print a summary of fitted model
print(rf_basic)
```


```{r}
# Evaluate the model on the testing subset of the data using confusionMatrix() function from the {caret} library

# Make classification predictions
pred <- predict(rf_basic, data = df_test, type = "response")$predictions
x <- as.factor(round(pred[,2])) # Use 0.5 as threshold
y <- df_test[, target] 

# Confusion matrix
conf_matrix <- caret::confusionMatrix(data = x, reference = y)
conf_matrix
```

### REPORT ANSWER: 5.1 - Simple model
Q: Evaluate the model on the testing subset of the data. Is the data balanced in terms of observed TRUE and FALSE values? What does this imply for the interpretation of the different metrics?

A: At first glance, the model looks promising. The overall accuracy score of the model is 0.76, meaning 76% of the outputs are correctly classified by the model. The sensitivity score is 0.80, meaning that the model correctly predicted 80% of true positives in the data, and the Positive Predictive Value (PPV) is 0.83, meaning that when the model predicts the positive class (class 0), it is correct about 83% of the time. 

However,  there is an imbalance in the data, as shown by the prevalence score of 0.65, meaning that 65% pf the data truly belongs to class 0, and 35% belongs to class 1. Since the distribution is not balanced 50/50, this has implications for the interpretation of key metrics. 
For example, the high accuracy score could be due to the model's better ability to predict the majority class, and when we look at accuracy in combination with the No Information Rate (NIR) which always predicts the majority class (class 0) of 0.65, the improvement in prediction from 65% to 76% is not actually so large. 

Additionally, the PPV (as well as the Negative predicted Value (NPV)) is dependent on the distribution, so the high PPV of 0.83 is likely so high in part because the PPV is a measure of precision for class 0, and here, class 0 is simply more common than class 1. 


Sensitivity and specificity should also be considered together. While the sensitivity, or rate at which the model is correctly identifying true 0 class, is high (80%), the specificity, or that rate at which the model is correctly identifying true class 1, is lower at 69%, meaning that again, the model is favouring the majority class (0).

In this case of class imbalance, it is more useful to look at metrics that take into account any imbalance, such as Balanced Accuracy and Cohenâ€™s Kappa. The Balance Accuracy, which averages sensitivity and specificity, is 0.74.This is slightly lower than our overall accuracy. Since the Balanced Accuracy gives the same weight to both classes, regardless of their frequency within the dataset, this shows us that the imbalance is inflating our overall accuracy.
Likewise, the Kappa statistic can be used to account for class imbalance by normalizing the error rate to what we would expect by random chance. Our Kappa is 0.48, which shows a moderate agreement of the model beyond what we would expect by random chance.


## 2.2 Variable selection
### 2.2.1 Variable importance
```{r}
# Let's run the basic model again but with recording the variable importance
rf_basic <- ranger(
  y = df_train[, target],     # target variable
  x = df_train[, predictors_all], # Predictor variables
  importance = "permutation", # Pick permutation to calculate variable importance
  seed = 42, # Specify seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1
) # Use all but one CPU core for quick model training

# Extract the variable importance and create a long tibble
vi_rf_basic <- rf_basic$variable.importance |>
  bind_rows() |>
  pivot_longer(cols = everything(), names_to = "variable")

# Plot variable importance, ordered by decreasing value
gg <- vi_rf_basic |>
  ggplot(aes(x = reorder(variable, value), y = value)) +
  geom_bar(stat = "identity", fill = "grey50", width = 0.75) +
  labs(
    y = "Change in OOB MSE after permutation",
    x = "",
    title = "Variable importance based on OOB"
  ) +
  theme_classic() +
  theme(
    axis.text.y = element_text(size = 6) 
  )+
  coord_flip()

# Display plot
gg
```

### 2.2.2 Variable selection
```{r}
set.seed(42)

# run the algorithm
bor <- Boruta::Boruta(
  y = df_train[, target],
  x = df_train[, predictors_all],
  maxRuns = 50, # Number of iterations. Set to 30 or lower if it takes too long
  num.threads = parallel::detectCores() - 1
)

# obtain results: a data frame with all variables, ordered by their importance
df_bor <- Boruta::attStats(bor) |>
  tibble::rownames_to_column() |>
  arrange(desc(meanImp))

# plot the importance result
ggplot(
  aes(
    x = reorder(rowname, meanImp),
    y = meanImp,
    fill = decision
  ),
  data = df_bor
) +
  geom_bar(stat = "identity", width = 0.75) +
  scale_fill_manual(values = c("grey30", "tomato", "grey70")) +
  labs(
    y = "Variable importance",
    x = "",
    title = "Variable importance based on Boruta"
  ) +
  theme_classic() +
  theme(
    axis.text.y = element_text(size = 6) 
  )+
  coord_flip()

```

```{r}
# get retained important variables
predictors_selected <- df_bor |>
  filter(decision == "Confirmed") |>
  pull(rowname)

length(predictors_selected)
```

```{r}
# re-train Random Forest model
rf_bor <- ranger(
  y = df_train[, target], # target variable
  x = df_train[, predictors_selected], # Predictor variables
  seed = 42, # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1,
  probability = TRUE
) # Use all but one CPU core for quick model training

# quick report and performance of trained model object
rf_bor
```

```{r}
# Evaluate the model on the test set wih rf_bor using confusionMatrix() function 

# Make classification predictions
pred_bor <- predict(rf_bor, data = df_test, type = "response")$predictions
x_bor <- as.factor(round(pred_bor[,2])) # Use 0.5 as threshold
y_bor <- df_test[, target] 

# Confusion matrix
conf_matrix_bor <- caret::confusionMatrix(data = x_bor, reference = y_bor)
conf_matrix_bor
```

```{r}
# Out-of-bag prediction error
oob_basic <- rf_basic$prediction.error
oob_bor <- rf_bor$prediction.error

cat("Basic model OOB prediction error:", oob_basic,
    ", Boruta model OOB prediction error:", oob_bor)
```

```{r}
# Save relevant data for model testing in the next chapter.
saveRDS(
  rf_bor,
  here("data/rf_for_waterlog100.rds")
)

saveRDS(
  df_train[, c(target, predictors_selected)],
  here("data/cal_for_waterlog100.rds")
)

saveRDS(
  df_test[, c(target, predictors_selected)],
  here("data/val_for_waterlog100.rds")
)
```

### REPORT ANSWER: 5.2 - Variable selection
Q: Repeat the model evaluation and compare the model performance on the test set with what was obtained with the model using all available covariates. Which model generalises better to unseen data?
Would the same model choice be made if we considered the OOB prediction error reported as part of the trained model object?

A: The model with the reduced set informed by the Boruta algorithm (rf_bor) has a higher overall Accuracy (0.78), Balanced Accuracy (0.76), and Kappa statistic (0.52) than the basic model with all covariates (rf_basic), indicating an improved model performance, with better agreement beyond what we would expect by random chance. Since the prevalence was not changed, this implies that the improvement is due to better prediction (less misclassification) and not due to class imbalance. 
The sensitivity, specificity, PPV, and NPV are also all higher in the Boruta model performance than the basic model, which indicates that the Boruta model is better at predicting both class 0 and class 1, and that both the positive and negative predictions are more reliable. Therefore, the Boruta model likely generalises better to unseen data.

When considering the Out of Bag (OOB) prediction error, the same model choice would be made because the basic model OOB prediction error is 0.21, and the Boruta model OOB prediction error is 0.14, and lower OOB prediction error typically indicates better generalisation to unseen data.


## 2.3 Model optimization
``` {r}
# Load the trained model, calibration and validation data 
rf_bor <- readRDS(here("data/rf_for_waterlog100.rds"))
df_train <- readRDS(here::here("data/cal_for_waterlog100.rds"))
df_test <- readRDS(here::here("data/val_for_waterlog100.rds"))

n_predictors <- length(predictors_selected) 

pp <- recipes::recipe(waterlog.100 ~ ., data = df_train)

# Set seed for randomization reproducibility 
set.seed(42)
rf_cv <- caret::train(
  pp, 
  data = df_train |> drop_na(),
  method = "ranger",             # RF method
  trControl = caret::trainControl(
    method = "cv", 
    number = 5
    ),
  tuneGrid = expand.grid(
    .mtry = 1:floor(n_predictors / 3),
    .min.node.size = c(2, 5, 10, 20),
    .splitrule = "gini"
  ),
  metric = "Accuracy",
  num.trees = 2000,
  replace = FALSE,
  sample.fraction = 0.5
)

print(rf_cv)

```

```{r}
# Train the best model
set.seed(42)
rf_best <- caret::train(
  pp, 
  data = df_train |> drop_na(),
  method = "ranger",             # RF method
  trControl = caret::trainControl(
    method = "cv", 
    number = 5
    ),
  tuneGrid = expand.grid(
    .mtry = 3,
    .min.node.size = 20,
    .splitrule = "gini"
  ),
  metric = "Accuracy",
  num.trees = 2000,
  replace = FALSE,
  sample.fraction = 0.5
)

print(rf_best)

```

```{r}
# Evaluate the model on the test set using confusionMatrix() function 

# Make predictions
x_pred_rfbest <- predict(rf_best, newdata = df_test)
y_best <- df_test[, target]


# Confusion matrix
conf_matrix_best<- caret::confusionMatrix(data = factor(x_pred_rfbest), reference = factor(y_best))
conf_matrix_best
```

### REPORT ANSWER: 5.3 - Model optimization
Q: Does the model generalise better to unseen data than the initial model

A: Yes, the optimized model (rf_best) generalises better to unseen data than the initial model (rf_basic). By optimizing the hyperparameters with mtry set at 3 and min.node.size set at 20, the rf_best model shows some slight improvements over rf_basic. For example, The overall Accuracy(0.765) and Balanced Accuracy (0.75) for rf_best are slightly higher than than those of rf-basic (0.76 and 0.74, respectively). The sensitivity remained the same between models (0.80) though specificity improved from rf_basic (0.69) to rf_best (0.71), indicating that rf_best more accurately identifies the negative class than rf_basic. Additionally, the Kappa statistic is slightly higher in rf_best (0.49) than rf_basic (0.48), indicating a slight improvement in agreement of rf_best between the observed and predicted values. Overall, these metrics support that rf_best may better generalise to unseen data than rf_basic. 


## 2.4 Probabilistic predictions
```{r}
# Re-train Random Forest model to predict probability of the target to be TRUE
rf_best <- ranger(
  y = df_train[, target],              # target variable
  x = df_train[, predictors_selected], # Predictor variables
  seed = 42,                           # Specify the seed for randomization to reproduce the same model again
  num.threads = parallel::detectCores() - 1, # Use all but one CPU core for quick model training
  mtry = 3,
  min.node.size = 20,  # value from optimization
  probability = TRUE
)
```

```{r}
pred_best <- predict(rf_best, data = df_test, type = "response")$predictions
x_best <- round(pred_best[,2]) # Using threshold as 0.5
y_best <- df_test[, target] 

# Create a ROC curve
roc_curve <- roc(y_best, x_best)

# Plot the ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2.5)
```

### REPORT ANSWER: 5.4 - Probabilistic predictions
Q: Consider you inform an infrastructure construction project where waterlogged soils severely jeopardize the stability of the building. Then, consider you inform a project where waterlogged soils are unwanted, but not critical. In both cases, your prediction map of a binary classification is used as a basis and the binary classification is derived from the probabilistic prediction. How would you chose the threshold in each case? Would you chose the same threshold in both cases? If not, explain why. Can you think of an analogy of a similarly-natured problem in another realm?


A: For the project where waterlogged soils severely jeopardize the stability of the building, the sensitivity is critical, so a lower threshold should be chosen. By prioritizing a low sensitivity threshold, this could reduce the risk of false negatives.
On the other hand, in the case of the project where waterlogged soils are unwanted but not critical, the threshold should be adjusted to find a balance between false positives and false-negatives. 

We can think of an analogous problem from the public health realm, and more specifically from the recent past when health scientists were developing the tests for what was at the time a new emerging infectious disease (COVID-19). If the test strips provided too many false negatives, then more people who truly were infected would believe that they were not, and contribute to further spread of the virus. In this case the sensitivity of the test (correctly identifying infected individuals as positive) was more important then the specificity (correctly identifying un-infected individuals as negative) because the risk of a false negative (misclassifying someone who truly was infected) had major repercussions for public health and the spread of the virus. Conversely, if we imagine there were a new treatment developed for COVID-19 which had major side-effects, we could imagine that the test specificity would become important, since misclassifying healthy individuals as infected could lead to unnecessary risks due to the treatment side-effects. 
